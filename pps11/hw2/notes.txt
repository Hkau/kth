# Matrix sums: gcc -DMAXSIZE=10000 -o matrixSum-openmp matrixSum-openmp.c -fopenmp && ./matrixSum-openmp

# I never used any arrays for partial sums, I used local variables within the block, so after I finished 1a I was done with 1b as well, hope that's ok. The master thread prints after the parallel block finishes.

# Running times (4 physical cores)

# n = 100

OMP_NUM_THREADS=1: 0.00013
OMP_NUM_THREADS=2: 0.00013
OMP_NUM_THREADS=3: 0.00013
OMP_NUM_THREADS=4: 0.00013

- Thoughts: For n=100, results aren't really worth presenting. Overhead from more threads seem to cancel out the benefits.

# n = 1000

OMP_NUM_THREADS=1: 0.007
OMP_NUM_THREADS=2: 0.0036
OMP_NUM_THREADS=3: 0.0025
OMP_NUM_THREADS=4: 0.0019

- Thoughts: It's worth branching into one extra thread, and the overhead from starting new threads isn't large compared to the benefits.

# n = 10000

OMP_NUM_THREADS=1: 0.71
OMP_NUM_THREADS=2: 0.36
OMP_NUM_THREADS=3: 0.24
OMP_NUM_THREADS=4: 0.18

- Thoughts: Same as above, multithreading shows great results.

# n = 20000
OMP_NUM_THREADS=1: 2.84
OMP_NUM_THREADS=2: 1.42
OMP_NUM_THREADS=3: 0.95
OMP_NUM_THREADS=4: 0.71

- Thoughts: Like n = 10000, this scales well.

# Performance
- Note that the program runs way slower than these times, because the real performance drop happens with rand() when the matrix is filled. If the goal was to optimize the program and not the algorithm, one'd best parallelize the initialization loop, which would require a pseudorandom number generator that is thread safe.
- When running on my laptop, with two cores, they showed good results for n>1000. The startup time etc. were way worse and the result became more stochastic, as thread overhead varied a lot.

# Quicksort: gcc -DARRAY_SIZE=10000000 -o qstest qs*.c -fopenmp && ./qstest

# n = 100 000

OMP_NUM_THREADS=1: 0.018
OMP_NUM_THREADS=2: 0.012
OMP_NUM_THREADS=3: 0.009
OMP_NUM_THREADS=4: 0.011

# n = 1 000 000

OMP_NUM_THREADS=1: 0.22
OMP_NUM_THREADS=2: 0.12
OMP_NUM_THREADS=3: 0.082
OMP_NUM_THREADS=4: 0.068

# n = 10 000 000

OMP_NUM_THREADS=1: 2.53
OMP_NUM_THREADS=2: 1.32
OMP_NUM_THREADS=3: 0.92
OMP_NUM_THREADS=4: 0.74

# n = 100 000 000

OMP_NUM_THREADS=1: 28.8
OMP_NUM_THREADS=2: 15.0
OMP_NUM_THREADS=3: 10.5
OMP_NUM_THREADS=4: 8.12

- Thoughts: Scales really well. The overhead for threads seems present in n = 100 000 and smaller, but for the other problems they scale well.


